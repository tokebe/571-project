---
title: "Cleaning"
author: "Hans Quiogue"
date: "3/20/2021"
output: html_document
---

```{r loading libraries}
suppressMessages(library(tidyverse))
suppressMessages(library(maps))
suppressMessages(library(datasets))
```

```{r reading in data}
# Reads in data 
raw_data <- read.csv('ufo_data.csv', stringsAsFactors = FALSE) 
```

```{r cleaning}
# Removes blank values in 'City' (no geo data to use)
raw_data <- raw_data[!(is.na(raw_data$City) | raw_data$City == ""), ]
```

```{r cleaning city and making region}
# New column for regions in 'City' column
raw_data$`Posted Region` <- NA

# Extract regions from text in paranthesis in 'City'
extracted_regions <- regmatches(raw_data$City, gregexpr("(?<=\\().+?(?=\\))", raw_data$City, perl = T))
# Converts null values to NA (for preservation)
extracted_regions[sapply(extracted_regions, function(x) length(x) == 0L)] <- NA
# Converts list to vector form
extracted_regions <- sapply(extracted_regions, toString)

# Place regions in user 'Posted Region'
raw_data$`Posted Region` <- extracted_regions

# City name without region in parenthesis
raw_data$City <- gsub("\\s*\\([^\\)]+\\)","", as.character(raw_data$City))

# Manually updating UK regions (there is so many I gave up, can probably use more regex above)
raw_data$`Posted Region`[raw_data$`Posted Region` == 'UK/England'] <- 'UK'
raw_data$`Posted Region`[raw_data$`Posted Region` == 'England'] <- 'UK'
raw_data$`Posted Region`[raw_data$`Posted Region` == 'UK/Wales'] <- 'UK'
raw_data$`Posted Region`[raw_data$`Posted Region` == 'UK/Scotland'] <- 'UK'
raw_data$`Posted Region`[raw_data$`Posted Region` == 'Northern Ireland'] <- 'Ireland'
raw_data$`Posted Region`[raw_data$`Posted Region` == 'Hampshire, UK/England'] <- 'UK'
```

```{r joining with maps data}
# Gets list of US cities without state abbreviations (from maps library)
us_cities_updated <- us.cities
us_cities_updated$name <- substr(us_cities_updated$name, 1, nchar(us_cities_updated$name) - 3)
# Manually update to matching Washington DC
us_cities_updated$name[us_cities_updated$name == 'WASHINGTON'] <- 'Washington, D.C.'

# Gets list of Canadian cities without state abbreviations (from maps library)
canada_cities_updated <- canada.cities
canada_cities_updated$name <- substr(canada_cities_updated$name, 1, nchar(canada_cities_updated$name) - 3)

# Dataframe of US with long/lat (reduced from 88k -> ~33K, can improve/increase by cleaning data  more before join)
us_data <- inner_join(raw_data, us_cities_updated, by = c("City" = "name", "State" = "country.etc"))
us_data$`Posted Region` <- "US"
# Dataframe of Canada with long/lat (only 62 values, can probably be improved by cleaning data more before join)
# Edit: (cleaning above increased data from 62 -> ~1900 entries)
canada_data <- inner_join(raw_data, canada_cities_updated, by = c("City" = "name", "State" = "country.etc"))
canada_data$`Posted Region` <- "Canada"
# Dataframe of 'international' countries
international_data <- inner_join(raw_data, world.cities, by = c("City" = "name", "Posted Region" = "country.etc"))

# Combines everything
data_with_coordinates <- rbind(us_data, canada_data)
data_with_coordinates <- rbind(data_with_coordinates, international_data)
```

```{r cleaning times}
# Clean time posted
data_with_coordinates$Posted <- as.Date(data_with_coordinates$Posted, format = "%m/%d/%y")

# Code below to convert to date and save hours + minutes as well (might not be accurate due to timezone issues..) 
# For now, I just converted to date without hours + minutes
# as.POSIXct(data_with_coordinates$Date.Time, format = "%m/%d/%y %H:%M")

# Clean time of sighting (only saves Dates)
data_with_coordinates$Date.Time <- as.Date(data_with_coordinates$Date.Time, format = "%m/%d/%y %H:%M")

# TODO: Duration?
```

```{r}
data_with_coordinates
```